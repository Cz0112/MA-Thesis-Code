{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.17.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2.2.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "# output csv file name\n",
    "CSV_FILENAME = 'gap_women_jeans.csv'\n",
    "# PLP stands for Product Listing Page\n",
    "# It is the page in which the list of products is available \n",
    "PLP_URL = \"https://www.gap.com/browse/category.do?cid=5664&nav=meganav%3AWomen%3ACategories%3AJeans#pageId=0&department=136\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    return: WebDriver instance (Chrome in this case)\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sleep to mimic human behavior\n",
    "def random_sleep(min_time=4, max_time=10):\n",
    "    \"\"\"\n",
    "    here uniform function is used which takes decimal values as well and not just whole numbers\n",
    "    thus showing somewhat more natural human behavior\n",
    "\n",
    "    :param min_time: minimum time to sleep, default value is 2\n",
    "    :param max_time: maximum time to sleep, default value is 5\n",
    "\n",
    "    delays program execution for a random amount of time between min_time and max_time\n",
    "    we use a range of 4-7 seconds as selenium sometimes requires quite a bit of time to load the page\n",
    "    \"\"\"\n",
    "    sleep_time = random.uniform(min_time, max_time)\n",
    "    sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll down the page to load more products\n",
    "def scroll(driver, timeout=30):\n",
    "    \"\"\"\n",
    "    Scrolls through the webpage to ensure all dynamic content is loaded.\n",
    "    :param driver: Selenium WebDriver instance\n",
    "    :param timeout: Time to wait for the page to load new content after scrolling\n",
    "    \"\"\"\n",
    "    scroll_pause_time = 2.5  # You can adjust this depending on how quickly the page loads\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait for new content to load\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # If heights are the same, it means there are no more products to load\n",
    "            break\n",
    "        last_height = new_height\n",
    "        # Optionally, add a random sleep to mimic human behavior\n",
    "        random_sleep(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BeautifulSoup object from the current page\n",
    "def get_soup(driver):\n",
    "    \"\"\"\n",
    "    :param driver: WebDriver instance\n",
    "    :return: BeautifulSoup object\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(driver.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product url from a product element\n",
    "def extract_pdp_url(product):\n",
    "    \"\"\"\n",
    "    pdp stands for product description page\n",
    "    it is the page in which the whole information about the product is \n",
    "    present \n",
    "    product url is present in the form - \n",
    "    'https://www.gap.com/browse/product.do? \n",
    " pid=774933022&cid=11900&pcid=11900&vid=1&nav=meganav%3AMen%3AJust%20Arrived%3ANew%20Arrivals&cpos=116&cexp=2859&kcid=CategoryIDs%3D11900&ctype=Listing&cpid=res23090805504869997736471#pdp-page-content'\n",
    "    we need to extract the part till the value of pid (inclusive)\n",
    "    the rest of the url is not needed and can even break the url at a \n",
    "    later date    \n",
    "\n",
    "    :param product: product element\n",
    "    :return: pdp url which is a string\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        url = product.find('a').get('href')\n",
    "        url = url.split('&')[0]\n",
    "    except:\n",
    "        url = 'Not available'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_demo(soup):\n",
    "    \"\"\"\n",
    "    Extracts the product demographic (e.g., men, women, boys, baby) from the product page.\n",
    "    The information is contained within the first <a> tag with a specific class.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: product demographic as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Directly find the first <a> tag with the specified class\n",
    "        product_demo_link = soup.find('a', class_='pdp-mfe-1lmagf7')\n",
    "        product_demo = product_demo_link.get_text() if product_demo_link else 'Not available'\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting product demographic: {e}\")\n",
    "        product_demo = 'error'\n",
    "\n",
    "    return product_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_type(soup):\n",
    "    \"\"\"\n",
    "    Extracts the product type (e.g., jeans, t-shirts, shirts) from the product page.\n",
    "    The information is contained within the second <a> tag with a specific class.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: product type as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all <a> tags with the specified class and select the second one for product type\n",
    "        product_type_links = soup.find_all('a', class_='pdp-mfe-1lmagf7')\n",
    "        product_type = product_type_links[1].get_text() if len(product_type_links) > 1 else 'Not available'\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting product type: {e}\")\n",
    "        product_type = 'error'\n",
    "\n",
    "    return product_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract product name from the product page\n",
    "def extract_product_name(soup):\n",
    "    \"\"\"\n",
    "    the h1 tag which contains the product has a different class name \n",
    "    for each product\n",
    "    but every h1 tag has the class name starting with pdp-mfe-\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: product name which is a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        product_name_element = soup.select('h1[class^=\"pdp-mfe-\"]') \n",
    "        product_name = product_name_element[0].text\n",
    "    except:\n",
    "        product_name = 'Not available'\n",
    "    return product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prices(soup):\n",
    "    \"\"\"\n",
    "    Extracts the selling price and max retail price from the product page,\n",
    "    accommodating different class identifiers for different website configurations.\n",
    "    \n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: selling price and max retail price as strings\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Try to find the selling price for both cases\n",
    "        selling_price_element = soup.find('div', class_=\"product-price--pdp__highlight\")\n",
    "        if not selling_price_element:\n",
    "            selling_price_element = soup.find('div', class_=\"pdp-pricing--highlight pdp-pricing__selected pdp-mfe-1jiw3bl\")\n",
    "        if not selling_price_element:  # Fallback to a more general search if specific classes fail\n",
    "            selling_price_element = soup.find('div', class_=re.compile(r'product-price|pdp-pricing'))\n",
    "        selling_price = selling_price_element.text.strip('$')\n",
    "        selling_price = re.sub(r'\\([^()]*\\)', '', selling_price).strip()\n",
    "        \n",
    "        # Try to find the max retail price for both cases\n",
    "        max_retail_price_element = soup.find('div', class_=\"product-price--pdp__regular\")\n",
    "        if not max_retail_price_element:\n",
    "            max_retail_price_element = soup.find('div', class_=\"product-price__strike pdp-mfe-vo1pn1\")\n",
    "        if max_retail_price_element:\n",
    "            max_retail_price = max_retail_price_element.text.strip('$')\n",
    "        else:\n",
    "            max_retail_price = selling_price  # Use the selling price if no separate max price is found\n",
    "    except AttributeError:  # Catch if any element is not found\n",
    "        selling_price = 'Not available'\n",
    "        max_retail_price = 'Not available'\n",
    "    \n",
    "    return selling_price, max_retail_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product rating from the product page\n",
    "def extract_star_value(soup):\n",
    "    \"\"\"\n",
    "    the span with class pdp-mfe-3jhqep contains the star rating in the \n",
    "    form - 5 stars, x are filled\n",
    "    we need to extract the value of x\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: star value which is a string\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        star_value = soup.find('span', class_='pdp-mfe-3jhqep').text\n",
    "        star_value = star_value.split(',')[1].split(' ')[1]\n",
    "    except:\n",
    "        star_value = 'Not available'\n",
    "    return star_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the number of product ratings from the product page\n",
    "def extract_ratings_count(soup):\n",
    "    \"\"\"\n",
    "    the div with class pdp-mfe-17iathi contains the number of ratings \n",
    "    in the form - x ratings\n",
    "    we need to extract the value of x\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: ratings count which is a string\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ratings_count = soup.find('div', class_='pdp-mfe-17iathi').text\n",
    "        ratings_count = ratings_count.split(' ')[0]\n",
    "    except:\n",
    "        ratings_count = 'Not available'\n",
    "    return ratings_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product color from the product page\n",
    "def extract_color(soup):\n",
    "    \"\"\"\n",
    "    the span with class swatch-label__value contains the color of the \n",
    "    product\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: color which is a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        color = soup.find('span', class_='swatch-label__value').text\n",
    "    except:\n",
    "        color = 'Not available'\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract available sizes from the product page\n",
    "def extract_available_sizes(soup):\n",
    "    \"\"\"\n",
    "    the div with class pdp-mfe-1kg10fj pdp-dimension pdp-dimension-- \n",
    "    should-display-redesign-in-stock contains the available sizes\n",
    "    the available sizes are stored into a list\n",
    "\n",
    "    in cases where there is no size available, the div with class pdp- \n",
    "    mfe-17f6z2a pdp-dimension pdp-dimension--should-display-redesign- \n",
    "    in-stock is not present\n",
    "    in such cases we return a list with 'Not applicable' as the only \n",
    "    element\n",
    "    this can be seen in case of accessories such as bags\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: available sizes which is a list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        available_sizes_element = soup.find_all('div', class_='pdp-mfe-1kg10fj pdp-dimension pdp-dimension--should-display-redesign-in-stock')\n",
    "        available_sizes = []\n",
    "        for size in available_sizes_element:\n",
    "            available_sizes.append(size.text)\n",
    "    except:\n",
    "        available_sizes = ['Not available']\n",
    "    if not available_sizes:\n",
    "        available_sizes = ['Not applicable']\n",
    "    return available_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product details from the product page\n",
    "def extract_details(soup):\n",
    "    \"\"\"\n",
    "    the product details are present in the form of a list\n",
    "    there are three sets of details - fit and sizing, product details, \n",
    "    fabric and care\n",
    "    each set of details is present in a ul tag with class name starting \n",
    "    with product-information-item__list\n",
    "    the text obtained is then normalized to remove any unicode \n",
    "    characters\n",
    "    normalizing means converting the special characters to their normal \n",
    "    form\n",
    "    in our case we can particularly see zero width space characters \n",
    "    (u200b) in the text \n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: fit and sizing, product details, fabric and care which are \n",
    "     lists\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        details_elements = soup.select('ul[class^=\"product-information-item__list\"]')\n",
    "        if len(details_elements) == 3:\n",
    "            fit_sizing_element = details_elements[0].find_all('li')\n",
    "            fit_sizing = []\n",
    "            for detail in fit_sizing_element:\n",
    "                if 'wearing' not in detail.text:\n",
    "                    text = unicodedata.normalize(\n",
    "                        \"NFKD\",\n",
    "                        detail.text\n",
    "                        ).rstrip('. ')\n",
    "                    fit_sizing.append(text)\n",
    "\n",
    "            product_details_element = details_elements[1].find_all('li')\n",
    "            product_details = []\n",
    "            for detail in product_details_element:\n",
    "                if '#' not in detail.text and  'P.A.C.E.' not in detail.text and 'pace' not in detail.text:\n",
    "                    text = unicodedata.normalize(\n",
    "                        \"NFKD\", \n",
    "                        detail.text\n",
    "                        ).rstrip('.')\n",
    "                    product_details.append(text)\n",
    "\n",
    "            fabric_care_element = details_elements[2].find_all('li')\n",
    "            fabric_care = []\n",
    "            for detail in fabric_care_element:\n",
    "                text = unicodedata.normalize(\n",
    "                    \"NFKD\", \n",
    "                    detail.text\n",
    "                    ).rstrip('. ')\n",
    "                fabric_care.append(text)\n",
    "\n",
    "        else:\n",
    "            fit_sizing = ['Not applicable']\n",
    "            \n",
    "            product_details_element = details_elements[0].find_all('li')\n",
    "            product_details = []\n",
    "            for detail in product_details_element:\n",
    "                if '#' not in detail.text and 'P.A.C.E.' not in detail.text and 'pace' not in detail.text:\n",
    "                    text = unicodedata.normalize(\n",
    "                        \"NFKD\", \n",
    "                        detail.text\n",
    "                        ).rstrip('.')\n",
    "                    product_details.append(text)\n",
    "            \n",
    "            fabric_care_element = details_elements[1].find_all('li')\n",
    "            fabric_care = []\n",
    "            for detail in fabric_care_element:\n",
    "                text = unicodedata.normalize(\n",
    "                    \"NFKD\", \n",
    "                    detail.text\n",
    "                    ).rstrip('. ')\n",
    "                fabric_care.append(text)\n",
    "                fabric_care.append(text)\n",
    "    except:\n",
    "        fit_sizing = ['Not available']\n",
    "        product_details = ['Not available']\n",
    "        fabric_care = ['Not available']\n",
    "    \n",
    "    return [fit_sizing, product_details, fabric_care]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed product 1\n",
      "Processed product 2\n",
      "Processed product 3\n",
      "Processed product 4\n",
      "Processed product 5\n",
      "Processed product 6\n",
      "Processed product 7\n",
      "Processed product 8\n",
      "Processed product 157\n",
      "Processed product 158\n",
      "Processed product 159\n",
      "Processed product 160\n",
      "Processed product 161\n",
      "Processed product 162\n",
      "Processed product 163\n",
      "Processed product 164\n",
      "Processed product 165\n",
      "Processed product 166\n",
      "Processed product 167\n",
      "Processed product 168\n",
      "Processed product 169\n",
      "Processed product 170\n",
      "Processed product 171\n",
      "Processed product 172\n",
      "Processed product 173\n",
      "Processed product 174\n",
      "Processed product 175\n",
      "Processed product 176\n",
      "Processed product 177\n",
      "Processed product 178\n",
      "Processed product 179\n",
      "Processed product 180\n",
      "Processed product 181\n",
      "Data written to gap_women_jeans.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    begins with initializing the WebDriver\n",
    "    then goes to the PLP_URL\n",
    "    - PLP means product listing page and it is the page in which the list of products is available\n",
    "\n",
    "    then scrolls down the page to load more products\n",
    "    then gets the BeautifulSoup object from the current page\n",
    "    then gets each product element from the main page\n",
    "    then extracts the product url from each product element and stores it in a list\n",
    "    then initializes a pandas dataframe with the required columns\n",
    "    then iterates through each product and extracts information\n",
    "    then stores the information in the initialized pandas dataframe\n",
    "    then prints the progress, which is the count of the current product\n",
    "    after going through every url writes the dataframe to the CSV file\n",
    "    then quits the WebDriver\n",
    "\n",
    "    in the above description each line corresponds to each section of the main function which is seperated by a blank line\n",
    "    \"\"\"\n",
    "    driver = initialize_driver()\n",
    "    driver.get(PLP_URL)\n",
    "\n",
    "    scroll(driver)\n",
    "\n",
    "    soup = get_soup(driver)\n",
    "    product_info = soup.find_all('div', class_='category-page-1wcebst')\n",
    "    pdp_url_list = [extract_pdp_url(product) for product in product_info]\n",
    "\n",
    "    df = pd.DataFrame(columns=\n",
    "                      ['Product_URL', 'Product_Demographic', 'Product_Type', 'Product_Name',\n",
    "                       'Selling_Price', 'Max_Retail_Price', 'Rating', \n",
    "                       'Rating_Count', 'Color', 'Available_Sizes', \n",
    "                       'Fit_Sizing', 'Product_Details', 'Fabric_Care']\n",
    "                       )\n",
    "        \n",
    "    for index, pdp_url in enumerate(pdp_url_list, start=1):\n",
    "        if pdp_url != 'Not available':\n",
    "            driver.get(pdp_url)\n",
    "            random_sleep()\n",
    "            soup = get_soup(driver)\n",
    "            product_demo = extract_product_demo(soup)\n",
    "            product_type = extract_product_type(soup)\n",
    "            product_name = extract_product_name(soup)\n",
    "            selling_price, max_retail_price = extract_prices(soup)\n",
    "            star_value = extract_star_value(soup)\n",
    "            ratings_count = extract_ratings_count(soup)\n",
    "            color = extract_color(soup)\n",
    "            available_sizes = extract_available_sizes(soup)\n",
    "            details = extract_details(soup)\n",
    "\n",
    "            df.loc[index] = [pdp_url, product_demo, product_type, product_name, selling_price, max_retail_price, star_value, ratings_count, color, ', '.join(available_sizes), *details]\n",
    "\n",
    "            print(f\"Processed product {index}\")\n",
    "\n",
    "    df.to_csv(CSV_FILENAME, index=False)\n",
    "    print(f\"Data written to {CSV_FILENAME}\")\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
