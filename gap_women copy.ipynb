{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.17.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2.2.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czxz6\\AppData\\Local\\Temp\\ipykernel_13696\\347365354.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "# output csv file name\n",
    "CSV_FILENAME = 'gap_women_Activewears.csv'\n",
    "# PLP stands for Product Listing Page\n",
    "# It is the page in which the list of products is available \n",
    "PLP_URL = \"https://www.gap.com/browse/category.do?cid=1117374&nav=meganav%3AWomen%3ACategories%3AGapFit%20Activewear#pageId=0&department=136\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.implicitly_wait(10)  # Implicit wait to handle dynamic content loading\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sleep to mimic human behavior\n",
    "def random_sleep(min_time=2, max_time=5):\n",
    "    \"\"\"\n",
    "    here uniform function is used which takes decimal values as well and not just whole numbers\n",
    "    thus showing somewhat more natural human behavior\n",
    "\n",
    "    :param min_time: minimum time to sleep, default value is 2\n",
    "    :param max_time: maximum time to sleep, default value is 5\n",
    "\n",
    "    delays program execution for a random amount of time between min_time and max_time\n",
    "    we use a range of 4-7 seconds as selenium sometimes requires quite a bit of time to load the page\n",
    "    \"\"\"\n",
    "    sleep_time = random.uniform(min_time, max_time)\n",
    "    sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll down the page to load more products\n",
    "def scroll(driver, timeout=30):\n",
    "    scroll_pause_time = 2.5\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        random_sleep(1, 3)  # Mimic human behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BeautifulSoup object from the current page\n",
    "def get_soup(driver):\n",
    "    \"\"\"\n",
    "    :param driver: WebDriver instance\n",
    "    :return: BeautifulSoup object\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(driver.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product url from a product element\n",
    "def extract_pdp_url(product):\n",
    "    \"\"\"\n",
    "    pdp stands for product description page\n",
    "    it is the page in which the whole information about the product is \n",
    "    present \n",
    "    product url is present in the form - \n",
    "    'https://www.gap.com/browse/product.do? \n",
    " pid=774933022&cid=11900&pcid=11900&vid=1&nav=meganav%3AMen%3AJust%20Arrived%3ANew%20Arrivals&cpos=116&cexp=2859&kcid=CategoryIDs%3D11900&ctype=Listing&cpid=res23090805504869997736471#pdp-page-content'\n",
    "    we need to extract the part till the value of pid (inclusive)\n",
    "    the rest of the url is not needed and can even break the url at a \n",
    "    later date    \n",
    "\n",
    "    :param product: product element\n",
    "    :return: pdp url which is a string\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        url = product.find('a').get('href')\n",
    "        url = url.split('&')[0]\n",
    "    except:\n",
    "        url = 'Not available'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_demo(soup):\n",
    "    \"\"\"\n",
    "    Extracts the product demographic (e.g., men, women, boys, baby) from the product page.\n",
    "    The information is contained within the first <a> tag with a specific class.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: product demographic as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Directly find the first <a> tag with the specified class\n",
    "        product_demo_link = soup.find('a', class_='pdp-mfe-1lmagf7')\n",
    "        product_demo = product_demo_link.get_text() if product_demo_link else 'Not available'\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting product demographic: {e}\")\n",
    "        product_demo = 'error'\n",
    "\n",
    "    return product_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_type(soup):\n",
    "    \"\"\"\n",
    "    Extracts the product type (e.g., jeans, t-shirts, shirts) from the product page.\n",
    "    The information is contained within the second <a> tag with a specific class.\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: product type as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all <a> tags with the specified class and select the second one for product type\n",
    "        product_type_links = soup.find_all('a', class_='pdp-mfe-1lmagf7')\n",
    "        product_type = product_type_links[1].get_text() if len(product_type_links) > 1 else 'Not available'\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting product type: {e}\")\n",
    "        product_type = 'error'\n",
    "\n",
    "    return product_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract product name from the product page\n",
    "def extract_product_name(soup):\n",
    "    \"\"\"\n",
    "    the h1 tag which contains the product has a different class name \n",
    "    for each product\n",
    "    but every h1 tag has the class name starting with pdp-mfe-\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: product name which is a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        product_name_element = soup.select('h1[class^=\"pdp-mfe-\"]') \n",
    "        product_name = product_name_element[0].text\n",
    "    except:\n",
    "        product_name = 'Not available'\n",
    "    return product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prices(soup):\n",
    "    \"\"\"\n",
    "    Extracts the selling price and max retail price from the product page,\n",
    "    accommodating different class identifiers for different website configurations.\n",
    "    \n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: selling price and max retail price as strings\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Try to find the selling price for both cases\n",
    "        selling_price_element = soup.find('div', class_=\"product-price--pdp__highlight\")\n",
    "        if not selling_price_element:\n",
    "            selling_price_element = soup.find('div', class_=\"pdp-pricing--highlight pdp-pricing__selected pdp-mfe-1jiw3bl\")\n",
    "        if not selling_price_element:  # Fallback to a more general search if specific classes fail\n",
    "            selling_price_element = soup.find('div', class_=re.compile(r'product-price|pdp-pricing'))\n",
    "        selling_price = selling_price_element.text.strip('$')\n",
    "        selling_price = re.sub(r'\\([^()]*\\)', '', selling_price).strip()\n",
    "        \n",
    "        # Try to find the max retail price for both cases\n",
    "        max_retail_price_element = soup.find('div', class_=\"product-price--pdp__regular\")\n",
    "        if not max_retail_price_element:\n",
    "            max_retail_price_element = soup.find('div', class_=\"product-price__strike pdp-mfe-vo1pn1\")\n",
    "        if max_retail_price_element:\n",
    "            max_retail_price = max_retail_price_element.text.strip('$')\n",
    "        else:\n",
    "            max_retail_price = selling_price  # Use the selling price if no separate max price is found\n",
    "    except AttributeError:  # Catch if any element is not found\n",
    "        selling_price = 'Not available'\n",
    "        max_retail_price = 'Not available'\n",
    "    \n",
    "    return selling_price, max_retail_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product rating from the product page\n",
    "def extract_star_value(soup):\n",
    "    \"\"\"\n",
    "    the span with class pdp-mfe-3jhqep contains the star rating in the \n",
    "    form - 5 stars, x are filled\n",
    "    we need to extract the value of x\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: star value which is a string\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        star_value = soup.find('span', class_='pdp-mfe-3jhqep').text\n",
    "        star_value = star_value.split(',')[1].split(' ')[1]\n",
    "    except:\n",
    "        star_value = 'Not available'\n",
    "    return star_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the number of product ratings from the product page\n",
    "def extract_ratings_count(soup):\n",
    "    \"\"\"\n",
    "    the div with class pdp-mfe-17iathi contains the number of ratings \n",
    "    in the form - x ratings\n",
    "    we need to extract the value of x\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: ratings count which is a string\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ratings_count = soup.find('div', class_='pdp-mfe-17iathi').text\n",
    "        ratings_count = ratings_count.split(' ')[0]\n",
    "    except:\n",
    "        ratings_count = 'Not available'\n",
    "    return ratings_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product color from the product page\n",
    "def extract_color(soup):\n",
    "    \"\"\"\n",
    "    the span with class swatch-label__value contains the color of the \n",
    "    product\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: color which is a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        color = soup.find('span', class_='swatch-label__value').text\n",
    "    except:\n",
    "        color = 'Not available'\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract available sizes from the product page\n",
    "def extract_available_sizes(soup):\n",
    "    \"\"\"\n",
    "    the div with class pdp-mfe-1kg10fj pdp-dimension pdp-dimension-- \n",
    "    should-display-redesign-in-stock contains the available sizes\n",
    "    the available sizes are stored into a list\n",
    "\n",
    "    in cases where there is no size available, the div with class pdp- \n",
    "    mfe-17f6z2a pdp-dimension pdp-dimension--should-display-redesign- \n",
    "    in-stock is not present\n",
    "    in such cases we return a list with 'Not applicable' as the only \n",
    "    element\n",
    "    this can be seen in case of accessories such as bags\n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: available sizes which is a list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        available_sizes_element = soup.find_all('div', class_='pdp-mfe-1kg10fj pdp-dimension pdp-dimension--should-display-redesign-in-stock')\n",
    "        available_sizes = []\n",
    "        for size in available_sizes_element:\n",
    "            available_sizes.append(size.text)\n",
    "    except:\n",
    "        available_sizes = ['Not available']\n",
    "    if not available_sizes:\n",
    "        available_sizes = ['Not applicable']\n",
    "    return available_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product details from the product page\n",
    "def extract_details(soup):\n",
    "    \"\"\"\n",
    "    the product details are present in the form of a list\n",
    "    there are three sets of details - fit and sizing, product details, \n",
    "    fabric and care\n",
    "    each set of details is present in a ul tag with class name starting \n",
    "    with product-information-item__list\n",
    "    the text obtained is then normalized to remove any unicode \n",
    "    characters\n",
    "    normalizing means converting the special characters to their normal \n",
    "    form\n",
    "    in our case we can particularly see zero width space characters \n",
    "    (u200b) in the text \n",
    "\n",
    "    :param soup: BeautifulSoup object\n",
    "    :return: fit and sizing, product details, fabric and care which are \n",
    "     lists\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        details_elements = soup.select('ul[class^=\"product-information-item__list\"]')\n",
    "        if len(details_elements) == 3:\n",
    "            fit_sizing_element = details_elements[0].find_all('li')\n",
    "            fit_sizing = []\n",
    "            for detail in fit_sizing_element:\n",
    "                if 'wearing' not in detail.text:\n",
    "                    text = unicodedata.normalize(\n",
    "                        \"NFKD\",\n",
    "                        detail.text\n",
    "                        ).rstrip('. ')\n",
    "                    fit_sizing.append(text)\n",
    "\n",
    "            product_details_element = details_elements[1].find_all('li')\n",
    "            product_details = []\n",
    "            for detail in product_details_element:\n",
    "                if '#' not in detail.text and  'P.A.C.E.' not in detail.text and 'pace' not in detail.text:\n",
    "                    text = unicodedata.normalize(\n",
    "                        \"NFKD\", \n",
    "                        detail.text\n",
    "                        ).rstrip('.')\n",
    "                    product_details.append(text)\n",
    "\n",
    "            fabric_care_element = details_elements[2].find_all('li')\n",
    "            fabric_care = []\n",
    "            for detail in fabric_care_element:\n",
    "                text = unicodedata.normalize(\n",
    "                    \"NFKD\", \n",
    "                    detail.text\n",
    "                    ).rstrip('. ')\n",
    "                fabric_care.append(text)\n",
    "\n",
    "        else:\n",
    "            fit_sizing = ['Not applicable']\n",
    "            \n",
    "            product_details_element = details_elements[0].find_all('li')\n",
    "            product_details = []\n",
    "            for detail in product_details_element:\n",
    "                if '#' not in detail.text and 'P.A.C.E.' not in detail.text and 'pace' not in detail.text:\n",
    "                    text = unicodedata.normalize(\n",
    "                        \"NFKD\", \n",
    "                        detail.text\n",
    "                        ).rstrip('.')\n",
    "                    product_details.append(text)\n",
    "            \n",
    "            fabric_care_element = details_elements[1].find_all('li')\n",
    "            fabric_care = []\n",
    "            for detail in fabric_care_element:\n",
    "                text = unicodedata.normalize(\n",
    "                    \"NFKD\", \n",
    "                    detail.text\n",
    "                    ).rstrip('. ')\n",
    "                fabric_care.append(text)\n",
    "                fabric_care.append(text)\n",
    "    except:\n",
    "        fit_sizing = ['Not available']\n",
    "        product_details = ['Not available']\n",
    "        fabric_care = ['Not available']\n",
    "    \n",
    "    return [fit_sizing, product_details, fabric_care]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\czxz6\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\czxz6\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\czxz6\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed product 1\n",
      "Processed product 2\n",
      "Processed product 3\n",
      "Processed product 4\n",
      "Processed product 5\n",
      "Processed product 6\n",
      "Processed product 7\n",
      "Processed product 8\n",
      "Processed product 9\n",
      "Processed product 10\n",
      "Processed product 11\n",
      "Processed product 12\n",
      "Processed product 13\n",
      "Processed product 14\n",
      "Processed product 15\n",
      "Processed product 16\n",
      "Processed product 17\n",
      "Processed product 18\n",
      "Processed product 19\n",
      "Processed product 20\n",
      "Processed product 21\n",
      "Processed product 22\n",
      "Processed product 23\n",
      "Processed product 24\n",
      "Processed product 25\n",
      "Processed product 26\n",
      "Processed product 27\n",
      "Processed product 28\n",
      "Processed product 29\n",
      "Processed product 30\n",
      "Processed product 31\n",
      "Processed product 32\n",
      "Processed product 33\n",
      "Processed product 34\n",
      "Processed product 35\n",
      "Processed product 36\n",
      "Processed product 37\n",
      "Processed product 38\n",
      "Processed product 39\n",
      "Processed product 40\n",
      "Processed product 41\n",
      "Processed product 42\n",
      "Processed product 43\n",
      "Processed product 44\n",
      "Processed product 45\n",
      "Processed product 46\n",
      "Processed product 47\n",
      "Processed product 48\n",
      "Processed product 49\n",
      "Processed product 50\n",
      "Processed product 51\n",
      "Processed product 52\n",
      "Processed product 53\n",
      "Processed product 54\n",
      "Processed product 55\n",
      "Processed product 56\n",
      "Processed product 58\n",
      "Processed product 59\n",
      "Processed product 60\n",
      "Processed product 61\n",
      "Processed product 62\n",
      "Processed product 63\n",
      "Processed product 64\n",
      "Processed product 65\n",
      "Processed product 66\n",
      "Processed product 67\n",
      "Processed product 68\n",
      "Processed product 69\n",
      "Processed product 70\n",
      "Processed product 71\n",
      "Processed product 72\n",
      "Processed product 73\n",
      "Processed product 74\n",
      "Processed product 75\n",
      "Data written to gap_women_Activewears.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    begins with initializing the WebDriver\n",
    "    then goes to the PLP_URL\n",
    "    - PLP means product listing page and it is the page in which the list of products is available\n",
    "\n",
    "    then scrolls down the page to load more products\n",
    "    then gets the BeautifulSoup object from the current page\n",
    "    then gets each product element from the main page\n",
    "    then extracts the product url from each product element and stores it in a list\n",
    "    then initializes a pandas dataframe with the required columns\n",
    "    then iterates through each product and extracts information\n",
    "    then stores the information in the initialized pandas dataframe\n",
    "    then prints the progress, which is the count of the current product\n",
    "    after going through every url writes the dataframe to the CSV file\n",
    "    then quits the WebDriver\n",
    "\n",
    "    in the above description each line corresponds to each section of the main function which is seperated by a blank line\n",
    "    \"\"\"\n",
    "    driver = initialize_driver()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(PLP_URL)\n",
    "\n",
    "    scroll(driver)\n",
    "\n",
    "    soup = get_soup(driver)\n",
    "    product_info = soup.find_all('div', class_='category-page-1wcebst')\n",
    "    pdp_url_list = [extract_pdp_url(product) for product in product_info]\n",
    "\n",
    "    # Ensure pdp_url_list contains unique and fully-loaded URLs\n",
    "    pdp_url_list = list(set(pdp_url_list))  # Remove duplicates if any\n",
    "\n",
    "    # Load existing data to check for duplicates\n",
    "    try:\n",
    "        existing_df = pd.read_csv(CSV_FILENAME)\n",
    "        existing_urls = existing_df['Product_URL'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        existing_urls = []\n",
    "\n",
    "    df = pd.DataFrame(columns=\n",
    "                      ['Product_URL', 'Product_Demographic', 'Product_Type', 'Product_Name',\n",
    "                       'Selling_Price', 'Max_Retail_Price', 'Rating', \n",
    "                       'Rating_Count', 'Color', 'Available_Sizes', \n",
    "                       'Fit_Sizing', 'Product_Details', 'Fabric_Care']\n",
    "                       )\n",
    "        \n",
    "    for index, pdp_url in enumerate(pdp_url_list, start=1):\n",
    "        if pdp_url != 'Not available':\n",
    "            driver.get(pdp_url)\n",
    "            random_sleep()\n",
    "            soup = get_soup(driver)\n",
    "            product_demo = extract_product_demo(soup)\n",
    "            product_type = extract_product_type(soup)\n",
    "            product_name = extract_product_name(soup)\n",
    "            selling_price, max_retail_price = extract_prices(soup)\n",
    "            star_value = extract_star_value(soup)\n",
    "            ratings_count = extract_ratings_count(soup)\n",
    "            color = extract_color(soup)\n",
    "            available_sizes = extract_available_sizes(soup)\n",
    "            details = extract_details(soup)\n",
    "\n",
    "            df.loc[index] = [pdp_url, product_demo, product_type, product_name, selling_price, max_retail_price, star_value, ratings_count, color, ', '.join(available_sizes), *details]\n",
    "\n",
    "            print(f\"Processed product {index}\")\n",
    "\n",
    "    # Concatenate the new data with existing data and drop duplicates\n",
    "    if not df.empty:\n",
    "        df = pd.concat([existing_df, df]).drop_duplicates(subset=['Product_URL'])\n",
    "        \n",
    "        df.to_csv(CSV_FILENAME, index=False)\n",
    "        print(f\"Data written to {CSV_FILENAME}\")\n",
    "        \n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed product 1\n",
      "Processed product 2\n",
      "Processed product 3\n",
      "Processed product 4\n",
      "Processed product 5\n",
      "Processed product 6\n",
      "Processed product 7\n",
      "Processed product 8\n",
      "Processed product 9\n",
      "Processed product 10\n",
      "Processed product 11\n",
      "Processed product 12\n",
      "Processed product 13\n",
      "Processed product 14\n",
      "Processed product 15\n",
      "Processed product 16\n",
      "Processed product 17\n",
      "Processed product 18\n",
      "Processed product 19\n",
      "Processed product 20\n",
      "Processed product 21\n",
      "Processed product 22\n",
      "Processed product 23\n",
      "Processed product 24\n",
      "Processed product 25\n",
      "Processed product 26\n",
      "Processed product 27\n",
      "Processed product 28\n",
      "Processed product 29\n",
      "Processed product 30\n",
      "Processed product 31\n",
      "Processed product 32\n",
      "Processed product 33\n",
      "Processed product 34\n",
      "Processed product 35\n",
      "Processed product 36\n",
      "Processed product 37\n",
      "Processed product 38\n",
      "Processed product 39\n",
      "Processed product 40\n",
      "Processed product 41\n",
      "Processed product 42\n",
      "Processed product 43\n",
      "Processed product 44\n",
      "Processed product 45\n",
      "Processed product 46\n",
      "Processed product 47\n",
      "Processed product 48\n",
      "Processed product 49\n",
      "Processed product 50\n",
      "Processed product 51\n",
      "Processed product 52\n",
      "Processed product 53\n",
      "Processed product 54\n",
      "Processed product 55\n",
      "Processed product 56\n",
      "Processed product 57\n",
      "Processed product 58\n",
      "Processed product 59\n",
      "Processed product 60\n",
      "Processed product 61\n",
      "Processed product 62\n",
      "Processed product 63\n",
      "Processed product 64\n",
      "Processed product 65\n",
      "Processed product 66\n",
      "Processed product 67\n",
      "Processed product 68\n",
      "Processed product 69\n",
      "Processed product 70\n",
      "Processed product 71\n",
      "Processed product 72\n",
      "Processed product 73\n",
      "Processed product 74\n",
      "Processed product 75\n",
      "Processed product 76\n",
      "Processed product 77\n",
      "Processed product 78\n",
      "Processed product 79\n",
      "Processed product 80\n",
      "Processed product 81\n",
      "Processed product 82\n",
      "Processed product 83\n",
      "Processed product 84\n",
      "Processed product 85\n",
      "Processed product 86\n",
      "Processed product 87\n",
      "Processed product 88\n",
      "Processed product 89\n",
      "Processed product 90\n",
      "Processed product 91\n",
      "Processed product 92\n",
      "Processed product 93\n",
      "Processed product 94\n",
      "Processed product 95\n",
      "Processed product 96\n",
      "Processed product 97\n",
      "Processed product 98\n",
      "Processed product 99\n",
      "Processed product 100\n",
      "Processed product 101\n",
      "Processed product 102\n",
      "Processed product 103\n",
      "Processed product 104\n",
      "Processed product 105\n",
      "Processed product 106\n",
      "Processed product 107\n",
      "Processed product 108\n",
      "Processed product 109\n",
      "Processed product 110\n",
      "Processed product 111\n",
      "Processed product 112\n",
      "Processed product 113\n",
      "Processed product 114\n",
      "Processed product 115\n",
      "Processed product 116\n",
      "Processed product 117\n",
      "Processed product 118\n",
      "Processed product 119\n",
      "Processed product 120\n",
      "Processed product 121\n",
      "Processed product 122\n",
      "Processed product 123\n",
      "Processed product 124\n",
      "Processed product 125\n",
      "Processed product 126\n",
      "Processed product 127\n",
      "Processed product 128\n",
      "Processed product 129\n",
      "Processed product 130\n",
      "Processed product 131\n",
      "Processed product 132\n",
      "Processed product 133\n",
      "Processed product 134\n",
      "Processed product 135\n",
      "Processed product 136\n",
      "Processed product 137\n",
      "Processed product 138\n",
      "Data written to gap_women_Activewears.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    begins with initializing the WebDriver\n",
    "    then goes to the PLP_URL\n",
    "    - PLP means product listing page and it is the page in which the list of products is available\n",
    "\n",
    "    then scrolls down the page to load more products\n",
    "    then gets the BeautifulSoup object from the current page\n",
    "    then gets each product element from the main page\n",
    "    then extracts the product url from each product element and stores it in a list\n",
    "    then initializes a pandas dataframe with the required columns\n",
    "    then iterates through each product and extracts information\n",
    "    then stores the information in the initialized pandas dataframe\n",
    "    then prints the progress, which is the count of the current product\n",
    "    after going through every url writes the dataframe to the CSV file\n",
    "    then quits the WebDriver\n",
    "\n",
    "    in the above description each line corresponds to each section of the main function which is seperated by a blank line\n",
    "    \"\"\"\n",
    "    driver = initialize_driver()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(PLP_URL)\n",
    "\n",
    "    scroll(driver)\n",
    "\n",
    "    soup = get_soup(driver)\n",
    "    product_info = soup.find_all('div', class_='category-page-1wcebst')\n",
    "    pdp_url_list = [extract_pdp_url(product) for product in product_info]\n",
    "\n",
    "    df = pd.DataFrame(columns=\n",
    "                      ['Product_URL', 'Product_Demographic', 'Product_Type', 'Product_Name',\n",
    "                       'Selling_Price', 'Max_Retail_Price', 'Rating', \n",
    "                       'Rating_Count', 'Color', 'Available_Sizes', \n",
    "                       'Fit_Sizing', 'Product_Details', 'Fabric_Care']\n",
    "                       )\n",
    "        \n",
    "    for index, pdp_url in enumerate(pdp_url_list, start=1):\n",
    "        if pdp_url != 'Not available':\n",
    "            driver.get(pdp_url)\n",
    "            random_sleep()\n",
    "            soup = get_soup(driver)\n",
    "            product_demo = extract_product_demo(soup)\n",
    "            product_type = extract_product_type(soup)\n",
    "            product_name = extract_product_name(soup)\n",
    "            selling_price, max_retail_price = extract_prices(soup)\n",
    "            star_value = extract_star_value(soup)\n",
    "            ratings_count = extract_ratings_count(soup)\n",
    "            color = extract_color(soup)\n",
    "            available_sizes = extract_available_sizes(soup)\n",
    "            details = extract_details(soup)\n",
    "\n",
    "            df.loc[index] = [pdp_url, product_demo, product_type, product_name, selling_price, max_retail_price, star_value, ratings_count, color, ', '.join(available_sizes), *details]\n",
    "\n",
    "            print(f\"Processed product {index}\")\n",
    "\n",
    "    df.to_csv(CSV_FILENAME, index=False)\n",
    "    print(f\"Data written to {CSV_FILENAME}\")\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
